---
title: User research with candidates on showing provider data round 2
description: Detailing the second round of user research with candidates about showing additional provider data
date: 2025-04-07
tags:
  - research
  - find
  - candidates
  - provider performance data
related:
  items:
    - text: User research with candidates on showing provider data
      href: https://becoming-a-teacher.design-history.education.gov.uk/find-teacher-training/user-research-with-candidates-on-showing-provider-data/
---

This is the second round of user research aimed at understanding what additional information candidates would find helpful about provider performance and how they would use it when deciding on a course.  

The first round of research is published [here](https://becoming-a-teacher.design-history.education.gov.uk/find-teacher-training/user-research-with-candidates-on-showing-provider-data/).

## Background of the research

The Find and Publish team needs to be confident in changes made to displaying provider performance data before this can be presented to providers.

This round of research will focus on reducing the additional information displayed in the designs on Find service based on the recommendations from the [first round of research]( https://becoming-a-teacher.design-history.education.gov.uk/find-teacher-training/user-research-with-candidates-on-showing-provider-data/).  

## Objectives

- To determine if there is a clear consensus from candidates on the usefulness of additional provider data and its use in finding appropriate courses to apply for  
- To identify and measure the impact that the success rate and response time has on candidates’ decision to apply for teacher training courses

## What we tested

This research aims to gather candidate feedback on provider performance data presented within the Find Teacher Training Course service.

The specific data tested included:  

- Qualified teacher status (QTS) success rate  
- Provider response time to applications  

## Participants

8 participants from inside and outside the UK.

However, one interview had to be cut short due to technical difficulties.

## Methodology

One-to-one interviews with domestic and international candidates via Microsoft Teams.

We completed testing by using a prototype version of the Find service, which allowed participants to interact with the service whilst being observed.

After the observation period, we asked participants specific questions to get their feedback on the Find service, and the data they are being shown.

## General findings

Participants liked using the prototype to compare teacher training courses.

However, there are some areas that could be improved to enhance their experience:

- Participants need clear visibility of training providers
- An easy way to compare courses
- A simple user interface
- Enhanced tools for filtering and user interaction
-

Participants compared courses using various criteria including:

- Entry requirements
- Course length
- Qualifications
-

Additionally, participants reported that they would use league tables and [Ofsted reports](https://reports.ofsted.gov.uk/) to identify potential red flags and challenging schools.

## Feedback on QTS success rate  

Participants consistently highlighted the importance of success rate data when selecting teacher training courses.

This feedback provides insight into how success rates influence decision-making, and the changes needed to improve usability.

### Key findings

The key finding have been categorised by relevance, clarity issues, data ambiguity, participant preference and accordion usage.

#### Relevance

Participants valued success rate data as a tool for identifying and shortlisting potential courses.

The average success rate was particularly useful for judging the overall standard of a course.

#### Clarity issues

Using phrases such as "under 80%" caused confusion, as participants were uncertain whether the actual rate was close to 80% or significantly lower.

Displaying exact percentages would provide clearer context and understanding.

#### Data ambiguity

It was unclear whether the success rate referred to the accredited provider or the training provider, adding to participant uncertainty.

#### Participant preferences

Courses with high success rates instilled greater confidence, while those with rates below 80% raised concerns.

Participants preferred courses that displayed their success rates and were discouraged by those without this information.

Displaying success rate data from a previous year was suggested as a better alternative to leaving it blank.

#### Accordion usage

Success rate average data was found helpful when accessed via the accordion component.  

### Implications for design

Participants’ feedback underscores the need for precise, transparent success rate data, which is clearly presented.
This is because enhanced clarity and visibility can significantly improve user confidence and aid in course selection.

## Feedback on the provider response time

Participants provided valuable insights into the role of response time in their decision-making process for selecting teacher training courses.

This feedback highlights the importance of clear communication and accurate presentation of response time data to enhance user experience.

### Key Findings

The key finding have been categorised by significance of response time, impact of longer response times, preferred response time rage and misinterpretations.

#### Significance of response time

Participants emphasized the usefulness of response time data for managing their personal schedules and factoring it into their decision-making. Knowing when a provider would respond to their application was considered essential information.

#### Impact of longer response times

Longer response times posed challenges, particularly for participants applying later in the application cycle. Timely responses are more critical as the cycle progresses.

#### Preferred response time range

While the average desired response time was 19 days, participants expressed a preference for response times within 1 to 2 weeks, highlighting a desire for quicker feedback.

#### Misinterpretations

Some participants misunderstood the meaning of response time data. For instance:

- One participant thought it referred to when a provider would receive their application
- Another interpreted it as the time taken for the website to respond to user actions (e.g., response to button clicking)

### Implications for design

- Clarification required: the definition of response time needs to be clearly articulated within the service to ensure participants can accurately interpret and use the information provided.
- Enhancing usability: aligning response time data with participants' preferences (e.g., shorter timeframes) and improving clarity can enhance decision-making and overall user confidence in the service

## Next steps

Before continuing with further user research on provider performance data, the team will need to:

- Review the data and how it is displayed on Find to ensure clarity for users
- Review the impact on surfacing this data to users in terms of whether it will aid candidates to better choose courses which they are more likely to receive an interview and offer.
