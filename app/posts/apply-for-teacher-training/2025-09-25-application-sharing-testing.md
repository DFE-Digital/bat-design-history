---
title: "Application sharing (find candidates) content testing"
description: Uncovering weaknesses in the application sharing content.
date: 2025-09-25
tags:
  - Find a candidate
  - Application sharing
  - User research
  - Content
---

## Background

Since launching application sharing, we have seen very good opt-in rates for candidates (18,378 – 39%). Candidates in previous rounds of research have also said that they think it is a positive thing and something that they are enthusiastic about because it’s seen as improving their chances of getting a place on a course.

However, past user research with candidates who have opted-in to application sharing demonstrated that they were not always clear about what they had signed up for. We also knew that certain concepts such as when a candidate is visible to training providers was not well understood.

The past research showed us there are some weaknesses in the content that we should try to understand more about.

## Highlighter testing

### Purpose

To specify which parts of the content are most and least clear

### Method

An unmoderated highlighter test sent to 13,837 candidates. 919 responded and they were a mix of domestic and international candidates (%?)  

They were shown sections of content from the application sharing journey and asked to highlight in:

* red any sentences that were confusing
* green any sentences that were clear
* yellow anything that was not needed

There was also a comprehension test at the end to check whether the content that people had said they understood matched the answers they got correct.  

### Results

**Concepts that were understood:**

* Accepting an invite to apply doesn't mean you will be accepted onto the course

> "If a training provider invites me to apply, I will be offered a place on their course."
> 82% of people responded false to this which is correct.

* Applications you submit following an invitation count towards your total

* You can opt-out if you want to

**Concepts that are not well understood:**

* When your application is visible to providers  

> "You have just submitted an application to a course. After your application is submitted, you are asked if you want to share your application details with other providers.  
> If you select 'yes' can providers can see your application details straight away?"
> 61% of people answered true to this question when it is false. This was the biggest misunderstanding in the highlighter testing.

* The name (application sharing)

* Setting location preferences (this is not explained in the content we tested so this is an expected result)

* When providers can invite you to apply (similar to when are you visible to providers)

> Training providers can invite me to apply at any time
> Just over half (51%) answered either true or I don’t know to this question which is false

* What data is visible in your profile to providers - especially past applications, personal statements and rejection reasons

**Concepts that did not do as well as we’d like them to:**

> "I do not need to search for courses and submit applications myself when I am opted in to application sharing"

This is false, we want people to continue applying proactively, but almost a third of respondents either did not know the answer or said this was true. This is a concern.

### Key things we learnt about highlighter testing

The take-up amongst candidates was much higher than expected. We predicted a 1% completion rate but we saw a 6.6% rate which was really encouraging. It's also worth noting that we did not offer incentives for this research task.

Once the test is set-up, it’s a quick way to get a view of key strengths and weaknesses of content – our responses came in over 7 days.

It’s a good idea to include a comprehension test in the highlighter test so that you can be sure that people really have understood the things they’ve said they do.

Highlighter testing can tell you what is not clear but not why. It’s a good idea to follow up with more contextual testing to understand exactly why the content isn’t performing well.  

## Unmoderated testing

### Purpose

To understand the ‘why’ of the things that were proven difficult to understand by the highlighter test.

### Method

Five candidates were sent a link to access the session through Lookback, where they were presented with a scenario followed by screenshots of a Figma prototype of the application sharing journey and asked to speak their thoughts aloud by following on-screen prompts.

This round had only 1 candidate currently living outside of the UK, but participants self-reported a mix of spoken English confidence.

### Results

Results were mixed due to a combination of the small sample size and some issues with the methodology.

However, the unmoderated testing bore out that the concept of when people are and are not visible to training providers is not clear to people. The landing page where we explain that whilst they have successfully opted in to share their details, they are not currently being shared because they have an application in flight with another training provider proved confusing for 3 out of 5 participants.  

Participants also felt that they should be visible all the time, they could not imagine a good reason for us structuring the feature the way we have:

> "The system should allow any other potential course provider to invite the user, no matter what the user status is."

Most participants also said they would want to click the ‘how this works’ link from this screen which further indicates a lack of confidence in what’s happening.

The ‘how application sharing works’ page was judged to be useful by participants who said that they like having content like this on a single page that they can easily return to if they need to. It wasn’t clear whether they had understood all of the content on the screen.

Several participants said that they would favour this way of being recruited over applying directly, reasons included:

* perceiving this as being quicker than waiting for a response to an application

* feeling that they have a better chance of being accepted onto a course they have been invited to

2 out of 5 people said that the ‘when can you be invited to apply’ information was the most important information on the page, and for one of them this was the first time they had understood this concept.

2 out of 5 people felt that the information about what data is and isn’t visible to providers was the least important information, but recognised this might be useful to other people. Their expectation was that all of their information would be shared and they were happy with this because they trust the DfE to protect their data.

### Key things we learnt about unmoderated testing

There were technical problems which could have been easily resolved in a moderated session. Two participants could not scroll down on Figma screenshots, one worked around this by resizing their browser, however it meant they couldn't see all of the 'how application sharing works’ content. Another could only see half of most screens so it’s difficult to use findings from that session.

The platform and prototype we used to do the unmoderated testing works best on desktop, this disadvantaged international candidates who are more likely to be accessing sites on their mobile. Lookback also appeared not to support the Safari browser causing participants to have to find workarounds (2 had to use a tablet or their phones for the session instead).

It’s challenging to make sure participants read and understand the test scenario, spend enough time on the tasks and answer them fully when you can’t prompt them. This meant that the on-screen instructions were quite long and repetitive. However, all of the participants spent at least 35 minutes on the activity.

This could be a useful methodology for smaller or simpler user journeys. It’s also better suited to interaction testing where it’s interesting to see what people do. With content testing we are trying to see what people understand, which is harder to do without a moderator to probe on specific points.  

## Moderated testing

### Purpose

This was an iteration of the unmoderated testing round to understand the ‘why’ of the highlighter testing. We hoped to overcome some of the drawbacks of the unmoderated round by testing with more people in moderated sessions.

### Method

Twelve participants were selected and invited to take part from the same screener used for the unmoderated session invites. Of these, eight participants signed up. A further two were invited but they did not respond, so there were in total eight participants (all showed up and engaged). There was a mix of domestic and international candidates at different stages of their application journey.

A virtual interview over the course of an hour where the participant clicked through a prototype of the application sharing journey.

### Results

* Fee-funded

Initially there was some confusion around the term ‘fee-funded’ which some participants understood as courses where the fees were funded by a third party.

![Asking if the user would consider fee-funded courses with radio buttons](fee-funded-question-v1.png)

We changed the wording to ‘courses with fees to pay’ midway through the round and this helped with understanding. We also changed the phrasing of the question because people were selecting ‘No I am only interested in salaried or apprenticeship routes into teaching’ when they meant that they would be open to both routes.  

![Asking if the user is interested in courses with fees to pay or salaried courses](fee-funded-question-v2.png)

We had originally had a warning component telling people that salaried routes fill up quickly and you’re unlikely to receive an invitation to such a course, but it had too much emphasis on the page and was the main focus for lots of candidates. We removed the warning component and moved the content into hint text under the salaried radio button.

![Asking if the user would consider fee-funded courses with a warning about salaried courses being rare](fee-funded-question-original.png)

* People value the information on the How it works page but 6 out of 8  participants did not click on it

People missed the link to the ‘How application sharing works’ page on the opt-in screen. Although some said that they wouldn’t click on it because they didn’t have any questions at this point, others would have found the information there useful to give them more confidence about what they were opting-in to.  

7 of 8 said they would opt-in because they trust the DfE and although they had some questions, they weren’t significant enough to stop them selecting ‘yes’. This supported the initial research that showed us that whilst people were opting-in, they did not always know what they had agreed to. 1 of the 8 misunderstood what they were being asked quite significantly and were going to opt-out on this basis until they read the ‘how it works’ page and corrected their misunderstanding.

When participants were directed to the content on the how application sharing works page, they agreed it was clear and useful. For some of them it clarified their confusion about when they are and are not visible to training providers.

We had a query about whether people need to know anything about their visibility at all, or if they can just wait to receive invitations when they become visible.

We feel we have enough insight now to say that this is important information for candidates. When the concept and reason was explained to them, several people said that they would value knowing this up-front because to wait without hearing from any providers when you think you are sharing your details with them is demoralising.  

* People were not clear that they were setting preferences for Application sharing and not an application

There was confusion amongst almost all participants when they were setting their location and funding-type preferences. When asked what the preferences were for, many of them said their applications, rather than their application sharing. The misconception that the preferences they were defining would apply to the courses they would apply to was common.

This could have been due to the scenario of the research session and the place in the journey where the prototype begins. It’s not very clear that they have just submitted an application and are now starting a new process, but we felt this was widespread enough amongst participants to record as a finding.

* People are confused about why they are not visible when they have an active application

This was the biggest issue throughout all 3 rounds of research. When candidates have an application that is in progress with another training provider (as all of them would at the point of opting in because it’s shown at the end of the application journey) they are hidden from the list that providers can see and are unable to receive invitations to apply.

Most found the journey they had just been on to share their application details contradicted the information they saw at the end that said they were not visible:

> "From what I've chosen previously, I have chosen that I want it visible, and now it is not visible.... I think that is... it's a bit confusing in this part."

When it was explained, some candidates said they would prefer to be visible all the time to increase their chances:

> "In my own opinion, I think other training providers should see my application just so that I don't miss out. Or there could be an option of training providers responding to applicants faster, so that if they don't find (me) a great fit, there'll be time for other providers to see the application and probably invite (me) to apply"

* Some people prefer to wait for invitations than apply proactively (especially given that submitting an application will hide them from providers)

This was an emerging finding from the highlighter testing which gained more weight in this moderated round. We understood that the reason people would stop proactively applying was because once they understood that an active application would hide them from other providers, they wanted to maximise their time being visible in the list. There was a perception that providers would be better able to choose candidates than candidates can choose courses, so there was a higher chance of success through application sharing than through applying proactively.

> "I will leave it at this stage... I won't submit at this point. I will leave them to see my application and contact me. I think it's better than submitting (myself)"

> “Well, I would be slightly confused. I was like, 'the way I understand, is that they can see my information and they will actively reach out to me...if I fit their requirements.' But then it says that I should not wait for this to happen and I should still proactively submit applications. I suppose I would be doing that anyway, but then I would wonder, 'So why did I just share that with you? If they reach out to me anyway, why do I still have to apply to them?'”

* Content on the how it works page could be clearer

The ‘how application sharing works’ page doesn’t contain a specific description of what application sharing is, we had assumed people would have some information about this when they arrive on the page, but the concept was much less clear to participants than we expected it to be – even if they’d opted-in.

Some participants who said that they valued the content about which parts of their applications are shared with providers noted that we don’t state when we stop sharing their data.

The heading ‘am I more likely to be successful’ didn’t really relate to the content beneath it, which was more about the importance of continuing to apply proactively.

Some participants also wanted to know what the process of being invited to apply was, with one specifying a step-by-step so their expectations were managed.

Some participants had misunderstandings regarding the answer to ‘how many applications can I submit’, getting confused about how this relates to application sharing specifically.

A few participants found the ‘When can I be invited to apply?’ answer a bit too long or convoluted and thought it could be conveyed in a more concise or simple way.

### Key things we learnt from the moderated testing

We tried to cover too much in terms of what we were getting candidate feedback on, meaning we could not go into sufficient depth within the hour sessions. Iterating and narrowing the scope halfway through data collection seemed to resolve this issue.

Having a moderator present was valuable in that it allowed us to be adaptable in probing specific issues or points of confusion to dig into what was causing them for each participant.
